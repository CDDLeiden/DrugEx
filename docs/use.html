<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Usage &mdash; DrugEx v3.4.4 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="drugex" href="api/modules.html" />
    <link rel="prev" title="Installation" href="install.html" />
 


<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YQZX39Y9Z8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-YQZX39Y9Z8');
</script>



</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> DrugEx
          </a>
              <div class="version">
                v3.4.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Welcome</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="#cli-example">CLI Example</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#basics">Basics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#fine-tuning-a-pretrained-generator">Fine-tuning a Pretrained Generator</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimization-with-reinforcement-learning">Optimization with Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#design-new-molecules">Design new molecules</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#advanced">Advanced</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#using-different-generator-architectures">Using different generator architectures</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#recurrent-neural-network">Recurrent neural network</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sequence-based-transformer">Sequence-based transformer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#pretraining-a-generator">Pretraining a Generator</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scaffold-based-reinforcement-learning">Scaffold-based Reinforcement learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#with-subset-of-molecules-containing-the-scaffold">With subset of molecules containing the scaffold</a></li>
<li class="toctree-l4"><a class="reference internal" href="#with-input-scaffold">With input scaffold</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api/modules.html">drugex</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">DrugEx</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Usage</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="usage">
<span id="id1"></span><h1>Usage<a class="headerlink" href="#usage" title="Permalink to this heading"></a></h1>
<p>In this document, the use of command line interface (CLI) will be described. If you want more control over the inputs and outputs or want to customize DrugEx itself, you can also use the Python API directly (see <span class="xref std std-ref">api-docs</span>). You can find a complete <a class="reference external" href="https://github.com/CDDLeiden/DrugEx/tree/master/tutorial">tutorial</a> illustrating some common use cases for each model type on the project’s GitHub.</p>
<p>The command-line is a simple interface that can be used to preprocess data and build models quickly. In order to obtain a final model and generate novel compounds you will need to run multiple scripts, however.
The description of the functionality of each script can be displayed with the <code class="code docutils literal notranslate"><span class="pre">--help</span></code> argument. For example, the help message for the <code class="code docutils literal notranslate"><span class="pre">drugex.dataset</span></code> script can be shown as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>drugex.dataset<span class="w"> </span>--help
</pre></div>
</div>
<p>On Linux and MacOS, you do not need to call python explicitly and the following will suffice:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>drugex<span class="w"> </span>dataset<span class="w"> </span>--help
</pre></div>
</div>
<p>A basic command-line workflow to fine-tune and optimize a graph-based model is given below (see <a class="reference internal" href="#cli-example"><span class="std std-ref">CLI Example</span></a>).
However, we also show a few other workflows to show some of the other functionalities.</p>
<p>Before you start, make sure you have downloaded the example data and models in the <code class="code docutils literal notranslate"><span class="pre">tutorial/CLI/examples</span></code> folder:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>drugex.download<span class="w"> </span>-o<span class="w"> </span>tutorial/CLI/examples<span class="w"> </span><span class="c1"># ran from the repository root</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All of the examples below also assume you are also executing them from the repository root.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>All of the commands below are intended as quick examples and it is unlikely the resulting models will be useful in any way. In production settings, the models should of course be trained for many more epochs.</p>
</div>
</section>
<section id="cli-example">
<span id="id2"></span><h1>CLI Example<a class="headerlink" href="#cli-example" title="Permalink to this heading"></a></h1>
<section id="basics">
<span id="id3"></span><h2>Basics<a class="headerlink" href="#basics" title="Permalink to this heading"></a></h2>
<section id="fine-tuning-a-pretrained-generator">
<h3>Fine-tuning a Pretrained Generator<a class="headerlink" href="#fine-tuning-a-pretrained-generator" title="Permalink to this heading"></a></h3>
<p>In this example, we will use the DrugEx CLI to fine-tune a pretrained graph transformer (trained on the latest version of the Papyrus data set).
This pretrained model has been trained on a diverse set of molecules.
Fine-tuning will give us a model that can generate molecules that should more closely resemble the compounds in the data set of interest.
You can find the model used here <a class="reference external" href="https://doi.org/10.5281/zenodo.7085421">archived on Zenodo</a> or among the other data files for this tutorial ‘CLI/generators/’.
You can find links to more pretrained models on the <a class="reference external" href="https://github.com/CDDLeiden/DrugEx">project GitHub</a>.</p>
<p>Here, we want to bias the model towards generating compounds that are more related to known ligands of the Adenosine receptors.
To use the CLI all the input data should be in the <code class="code docutils literal notranslate"><span class="pre">data</span></code> folder of the base directory <code class="code docutils literal notranslate"><span class="pre">-b</span> <span class="pre">tutorial/CLI</span></code>.
For fine-tuning this input is a file with compounds (<code class="code docutils literal notranslate"><span class="pre">A2AR_LIGANDS.tsv</span></code>)
Before we begin the fine-tuning, we have to preprocess the training data, as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># input is in tutorial/CLI/examples/data/A2AR_LIGANDS.tsv</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">BASE_DIR</span><span class="o">=</span>tutorial/CLI/examples
python<span class="w"> </span>-m<span class="w"> </span>drugex.dataset<span class="w"> </span>-b<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span><span class="w"> </span>-i<span class="w"> </span>A2AR_LIGANDS.tsv<span class="w"> </span>-mc<span class="w"> </span>SMILES<span class="w"> </span>-o<span class="w"> </span>arl<span class="w"> </span>-mt<span class="w"> </span>graph
</pre></div>
</div>
<p>This will tell DrugEx to preprocess compounds saved in the <code class="code docutils literal notranslate"><span class="pre">-mc</span> <span class="pre">SMILES</span></code> column of the <code class="code docutils literal notranslate"><span class="pre">-i</span> <span class="pre">A2AR_LIGANDS.tsv</span></code> file for a <code class="code docutils literal notranslate"><span class="pre">-mt</span> <span class="pre">graph</span></code> type transformer</p>
<p>Preprocessing molecules for the graph based models includes fragmentation and encoding. This is done because the transformer takes fragmented molecules as input.
For the graph-based transformers these inputs also need to be encoded into a graph representation.</p>
<p>The resulting files will be saved in the data folder and given a prefix (<code class="code docutils literal notranslate"><span class="pre">-o</span> <span class="pre">arl</span></code>). You can use this prefix to load the compiled data files in the next step. If you made and error somewhere or got an exception, you may also notice some <code class="code docutils literal notranslate"><span class="pre">backup_{number}</span></code> folders being created in the data folder. These are backups of the data files before the last step. You can use them to go back to the previous results if you accidentally overwrite them.</p>
<p>Now that we have our data sets prepared, we can finetune the pretrained generator on the preprocessed molecules with the <code class="code docutils literal notranslate"><span class="pre">train</span></code> script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>drugex.train<span class="w"> </span>-tm<span class="w"> </span>FT<span class="w"> </span>-b<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span><span class="w"> </span>-i<span class="w"> </span>arl<span class="w"> </span>-o<span class="w"> </span>arl<span class="w"> </span>-ag<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span>/models/pretrained/graph-trans/Papyrus05.5_graph_trans_PT/Papyrus05.5_graph_trans_PT.pkg<span class="w"> </span>-mt<span class="w"> </span>graph<span class="w"> </span>-e<span class="w"> </span><span class="m">2</span><span class="w"> </span>-bs<span class="w"> </span><span class="m">32</span><span class="w"> </span>-gpu<span class="w"> </span><span class="m">0</span>
</pre></div>
</div>
<p>This tells DrugEx to use the generated file (prefixed with <code class="code docutils literal notranslate"><span class="pre">-i</span> <span class="pre">arl</span></code>) to fine-tune (<code class="code docutils literal notranslate"><span class="pre">-m</span> <span class="pre">FT</span></code>) a pretrained model with model states saved in the <code class="code docutils literal notranslate"><span class="pre">-pt</span> <span class="pre">Papyrus05.5_graph_trans_PT.pkg</span></code> file.
The training will only be 2 epochs, <code class="code docutils literal notranslate"><span class="pre">-e</span> <span class="pre">2</span></code>, with a batch size of 32, <code class="code docutils literal notranslate"><span class="pre">-bs</span> <span class="pre">32</span></code> and it will be done on GPU 0, <code class="code docutils literal notranslate"><span class="pre">-gpu</span> <span class="pre">0</span></code>. You can also specify multiple GPUs with the <code class="code docutils literal notranslate"><span class="pre">-gpu</span></code> argument (i.e <code class="code docutils literal notranslate"><span class="pre">-gpu</span> <span class="pre">0,1</span></code>). The best model will be saved to <code class="code docutils literal notranslate"><span class="pre">${BASE_DIR}/generators/arl_graph_trans_FT.pkg</span></code>. However, you will find more output files with the <code class="code docutils literal notranslate"><span class="pre">.log</span></code> and <code class="code docutils literal notranslate"><span class="pre">.tsv</span></code> extensions in <code class="code docutils literal notranslate"><span class="pre">${BASE_DIR}</span></code>. These files contain the training and validation losses and the molecules generated at each epoch.</p>
</section>
<section id="optimization-with-reinforcement-learning">
<h3>Optimization with Reinforcement Learning<a class="headerlink" href="#optimization-with-reinforcement-learning" title="Permalink to this heading"></a></h3>
<p>In this example, want to generate drug-like molecules that are active on A2AR and have a high Syntehtic Accessibility Score (SAScore).
To achieve this, reinforcement learning (RL) is used to tune the generator model to generate molecules with desired properties.
For this task the RL framework is composed of the agent (generator) and environment (predictor and SAScorer).
The predictor model (a Random Forest QSAR model for binary A2A bioactivity predictions) has been <a class="reference external" href="https://github.com/CDDLeiden/QSPRPred">created using QSPRpred</a></p>
<p>During RL a combination of two generators with the same architecture is used to create molecules; the agent that is optimized during RL for exploitation and
the prior that is kept fixed for exploration.
At each iteration, generated molecules are scored based on the environment and send a back to the agent for tuning.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>drugex.train<span class="w"> </span>-tm<span class="w"> </span>RL<span class="w"> </span>-b<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span><span class="w"> </span>-i<span class="w"> </span>arl<span class="w"> </span>-o<span class="w"> </span>arl<span class="w"> </span>-ag<span class="w"> </span>arl_graph_trans_FT<span class="w"> </span>-pr<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span>/models/pretrained/graph-trans/Papyrus05.5_graph_trans_PT/Papyrus05.5_graph_trans_PT.pkg<span class="w"> </span>-p<span class="w"> </span>models/qsar/qspr/models/A2AR_RandomForestClassifier/A2AR_RandomForestClassifier_meta.json<span class="w"> </span>-ta<span class="w"> </span>A2AR_RandomForestClassifier<span class="w"> </span>-sas<span class="w"> </span>-e<span class="w"> </span><span class="m">2</span><span class="w"> </span>-bs<span class="w"> </span><span class="m">32</span><span class="w"> </span>-gpu<span class="w"> </span><span class="m">0</span>
</pre></div>
</div>
<p>This tells DrugEx to create molecules from input fragments encoded in preprocessed data file (prefixed with <code class="code docutils literal notranslate"><span class="pre">arl</span></code>)
and optimize the initial agent (the fine-tuned model) (<code class="code docutils literal notranslate"><span class="pre">-ag</span> <span class="pre">arl_graph_trans_FT</span></code>) with RL (<code class="code docutils literal notranslate"><span class="pre">-m</span> <span class="pre">RL</span></code>). In this case we are using two desirability functions to score molecules:</p>
<ul class="simple">
<li><p><strong>Pretrained QSAR Model</strong> (<code class="code docutils literal notranslate"><span class="pre">-p</span> <span class="pre">.../A2AR_RandomForestClassifier_meta.json</span></code>): This model is located in the <code class="code docutils literal notranslate"><span class="pre">tutorial/CLI/examples/models/qsar/</span></code> folder and is used to predict the bioactivity of the generated molecules on A2AR, which is indicated by adding it by name to the list of active targets with <code class="code docutils literal notranslate"><span class="pre">-ta</span> <span class="pre">A2AR_RandomForestClassifier</span></code>. This model was build using the <code class="code docutils literal notranslate"><span class="pre">QSPRpred</span></code> package and you can check out the Jupyter Notebook used to create it in the Python <a class="reference external" href="https://github.com/CDDLeiden/DrugEx/tree/master/tutorial/qsar.ipynb">tutorial</a></p></li>
<li><p><strong>SAScore</strong> (<code class="code docutils literal notranslate"><span class="pre">-sas</span></code>): This is a synthetic accessibility score that will prevent DrugEx from generating molecules that are too difficult to synthesize.</p></li>
</ul>
<p>The rate between exploration and exploitation of known chemical space is forced by the use of a fixed prior-generator (<code class="code docutils literal notranslate"><span class="pre">-pr</span> <span class="pre">Papyrus05.5_graph_trans_PT</span></code>) and its influence can be tuned with the <code class="code docutils literal notranslate"><span class="pre">-eps,</span> <span class="pre">--epsilon</span></code> parameter.
The best model found during RL will be saved as <code class="code docutils literal notranslate"><span class="pre">${BASE_DIR}/generators/arl_graph_trans_RL.pkg</span></code>.</p>
</section>
<section id="design-new-molecules">
<h3>Design new molecules<a class="headerlink" href="#design-new-molecules" title="Permalink to this heading"></a></h3>
<p>In this example, we use the optimized agent model to design new compounds that should be active on A2AR and have high synthetic accessibility.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>drugex.generate<span class="w"> </span>-b<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span><span class="w"> </span>-i<span class="w"> </span>arl_test_graph.txt<span class="w"> </span>-g<span class="w"> </span>arl_graph_trans_RL
</pre></div>
</div>
<p>This tells DrugEx to generate new molecules based on the input fragment in <code class="code docutils literal notranslate"><span class="pre">arl_test_graph.txt</span></code> with the <code class="code docutils literal notranslate"><span class="pre">arl_graph_trans_RL.pkg</span></code> model.
The new compounds are saved to <code class="code docutils literal notranslate"><span class="pre">${BASE_DIR}/new_molecules/arl_graph_trans_RL.tsv</span></code> and are also scored with the original environment used to create the model.</p>
</section>
</section>
<section id="advanced">
<h2>Advanced<a class="headerlink" href="#advanced" title="Permalink to this heading"></a></h2>
<section id="using-different-generator-architectures">
<h3>Using different generator architectures<a class="headerlink" href="#using-different-generator-architectures" title="Permalink to this heading"></a></h3>
<p>You can vary the type of model to use with the <code class="code docutils literal notranslate"><span class="pre">-a</span></code> and <code class="code docutils literal notranslate"><span class="pre">-mt</span></code> parameters.</p>
<section id="recurrent-neural-network">
<h4>Recurrent neural network<a class="headerlink" href="#recurrent-neural-network" title="Permalink to this heading"></a></h4>
<p>The most simple model is the RNN-based generator. This model gets the ‘go’ token as input and from there generates SMILES strings.
Therefore, this model does not use input fragments for training or sampling. To preprocess the data for training an RNN-based generator the molecules
are standardized and encoded based on the vocabulary of the pretrained model <code class="code docutils literal notranslate"><span class="pre">-vf</span> <span class="pre">Papyrus05.5_smiles_voc.txt</span></code>, but no fragmentation is done <code class="code docutils literal notranslate"><span class="pre">-nof</span></code>.
To fine-tune an RNN-based generator on the A2AR set, the algorithm needs to be specified <code class="code docutils literal notranslate"><span class="pre">-a</span> <span class="pre">rnn</span></code>.
Here the generator is fine-tuned on the A2AR set and then used to generate new compounds.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>drugex.dataset<span class="w"> </span>-b<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span><span class="w"> </span>-i<span class="w"> </span>A2AR_LIGANDS.tsv<span class="w"> </span>-mc<span class="w"> </span>SMILES<span class="w"> </span>-o<span class="w"> </span>rnn-example<span class="w"> </span>-nof<span class="w"> </span>-vf<span class="w"> </span>Papyrus05.5_smiles_voc.txt
python<span class="w"> </span>-m<span class="w"> </span>drugex.train<span class="w"> </span>-tm<span class="w"> </span>FT<span class="w"> </span>-b<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span><span class="w"> </span>-i<span class="w"> </span>rnn-example<span class="w"> </span>-ag<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span>/models/pretrained/smiles-rnn/Papyrus05.5_smiles_rnn_PT/Papyrus05.5_smiles_rnn_PT.pkg<span class="w"> </span>-vfs<span class="w"> </span>Papyrus05.5_smiles_voc.txt<span class="w"> </span>-mt<span class="w"> </span>smiles<span class="w"> </span>-a<span class="w"> </span>rnn<span class="w"> </span>-e<span class="w"> </span><span class="m">2</span><span class="w"> </span>-bs<span class="w"> </span><span class="m">32</span><span class="w"> </span>-gpu<span class="w"> </span><span class="m">0</span>
python<span class="w"> </span>-m<span class="w"> </span>drugex.generate<span class="w"> </span>-b<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span><span class="w"> </span>-g<span class="w"> </span>rnn-example_smiles_rnn_FT<span class="w"> </span>-vfs<span class="w"> </span>Papyrus05.5_smiles_voc.txt<span class="w"> </span>-gpu<span class="w"> </span><span class="m">0</span><span class="w"> </span>-n<span class="w"> </span><span class="m">30</span><span class="w"> </span>--keep_undesired
</pre></div>
</div>
</section>
<section id="sequence-based-transformer">
<h4>Sequence-based transformer<a class="headerlink" href="#sequence-based-transformer" title="Permalink to this heading"></a></h4>
<p>For working with a SMILES-based transformer; you need to preprocess the data by specifying <code class="code docutils literal notranslate"><span class="pre">-mt</span> <span class="pre">smiles</span></code> indicating that the inputs are encoded as SMILES.
By default the transformer algorithm (<code class="code docutils literal notranslate"><span class="pre">-a</span> <span class="pre">trans</span></code>) is used for training.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Note that the pretrained model for this model is not fetched by the tutorial utility at this point so you will have download its files separately. This model is also still more experimental and will likely not perform as well as the previous models.</p>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>drugex.dataset<span class="w"> </span>-b<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span><span class="w"> </span>-i<span class="w"> </span>A2AR_LIGANDS.tsv<span class="w"> </span>-mc<span class="w"> </span>SMILES<span class="w"> </span>-o<span class="w"> </span>ast<span class="w"> </span>-mt<span class="w"> </span>smiles
python<span class="w"> </span>-m<span class="w"> </span>drugex.train<span class="w"> </span>-tm<span class="w"> </span>FT<span class="w"> </span>-i<span class="w"> </span>ast<span class="w"> </span>-ag<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span>/models/pretrained/smiles-trans/Papyrus05.5_smiles_trans_PT/Papyrus05.5_smiles_trans_PT.pkg<span class="w"> </span>-mt<span class="w"> </span>smiles<span class="w"> </span>-a<span class="w"> </span>trans<span class="w"> </span>-e<span class="w"> </span><span class="m">2</span><span class="w"> </span>-bs<span class="w"> </span><span class="m">32</span><span class="w"> </span>-gpu<span class="w"> </span><span class="m">0</span>
</pre></div>
</div>
</section>
</section>
<section id="pretraining-a-generator">
<h3>Pretraining a Generator<a class="headerlink" href="#pretraining-a-generator" title="Permalink to this heading"></a></h3>
<p>Pretraining <code class="code docutils literal notranslate"><span class="pre">-m</span> <span class="pre">PT</span></code> of a model from scratch works exactly the same way as finetuning,
the only difference is that the generator will not be initialized with pretrained model weights.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>drugex.dataset<span class="w"> </span>-b<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span><span class="w"> </span>-i<span class="w"> </span>A2AR_LIGANDS.tsv<span class="w"> </span>-mc<span class="w"> </span>SMILES<span class="w"> </span>-o<span class="w"> </span>example_pt<span class="w"> </span>-mt<span class="w"> </span>graph
python<span class="w"> </span>-m<span class="w"> </span>drugex.train<span class="w"> </span>-tm<span class="w"> </span>PT<span class="w"> </span>-b<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span><span class="w"> </span>-i<span class="w"> </span>example_pt<span class="w"> </span>-mt<span class="w"> </span>graph<span class="w"> </span>-e<span class="w"> </span><span class="m">2</span><span class="w"> </span>-bs<span class="w"> </span><span class="m">32</span><span class="w"> </span>-gpu<span class="w"> </span><span class="m">0</span>
</pre></div>
</div>
</section>
<section id="scaffold-based-reinforcement-learning">
<h3>Scaffold-based Reinforcement learning<a class="headerlink" href="#scaffold-based-reinforcement-learning" title="Permalink to this heading"></a></h3>
<p>Tuning of the transformer-based generators can also be done on one scaffold or a subset of scaffolds. There are two ways to do it, either by using a subset of fragments-molecule pairs containing the selected scaffold or using the directly the scaffold as input. If your training data contains molecules with the selected scaffold we recommend former methods as its more stable with policy gradient-based reinforcement learning.</p>
<p>Here we show examples of these approaches on the previously trained and fine-tuned A2AR generators. We will use the molecule xanthine as a scaffold, in both examples.</p>
<section id="with-subset-of-molecules-containing-the-scaffold">
<h4>With subset of molecules containing the scaffold<a class="headerlink" href="#with-subset-of-molecules-containing-the-scaffold" title="Permalink to this heading"></a></h4>
<p>First the molecules from the given dataset are fragmented and encoding while only selecting fragments-molecule pairs (<code class="code docutils literal notranslate"><span class="pre">-s</span> <span class="pre">&lt;scaffold&gt;</span></code>) containing the xanthine in the input fragements, then we proceed with RL with this subset of molecules.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>drugex.dataset<span class="w"> </span>-b<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span><span class="w"> </span>-i<span class="w"> </span>A2AR_LIGANDS.tsv<span class="w"> </span>-mc<span class="w"> </span>SMILES<span class="w"> </span>-o<span class="w"> </span>arl_xanthine<span class="w"> </span>-mt<span class="w"> </span>graph<span class="w"> </span>-sf<span class="w"> </span>c1<span class="o">[</span>nH<span class="o">]</span>c2c<span class="o">(</span>n1<span class="o">)</span>nc<span class="o">(</span>nc2O<span class="o">)</span>O
python<span class="w"> </span>-m<span class="w"> </span>drugex.train<span class="w"> </span>-tm<span class="w"> </span>RL<span class="w"> </span>-b<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span><span class="w"> </span>-i<span class="w"> </span>arl_xanthine<span class="w"> </span>-o<span class="w"> </span>arl_xanthine<span class="w"> </span>-ag<span class="w"> </span>arl_graph_trans_FT<span class="w"> </span>-pr<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span>/models/pretrained/graph-trans/Papyrus05.5_graph_trans_PT/Papyrus05.5_graph_trans_PT.pkg<span class="w"> </span>-p<span class="w"> </span>models/qsar/qspr/models/A2AR_RandomForestClassifier/A2AR_RandomForestClassifier_meta.json<span class="w"> </span>-ta<span class="w"> </span>A2AR_RandomForestClassifier<span class="w"> </span>-sas<span class="w"> </span>-e<span class="w"> </span><span class="m">2</span><span class="w"> </span>-bs<span class="w"> </span><span class="m">32</span><span class="w"> </span>-gpu<span class="w"> </span><span class="m">0</span>
python<span class="w"> </span>-m<span class="w"> </span>drugex.generate<span class="w"> </span>-b<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span><span class="w"> </span>-i<span class="w"> </span>arl_xanthine<span class="w"> </span>-g<span class="w"> </span>arl_xanthine_graph_trans_RL<span class="w"> </span>-gpu<span class="w"> </span><span class="m">0</span><span class="w"> </span>-n<span class="w"> </span><span class="m">5</span>
</pre></div>
</div>
<p>If you want the fragments-molecule pairs consist of ones with exclusively the selected scaffold as the input fragment add the argument <code class="code docutils literal notranslate"><span class="pre">-sfe</span></code></p>
</section>
<section id="with-input-scaffold">
<h4>With input scaffold<a class="headerlink" href="#with-input-scaffold" title="Permalink to this heading"></a></h4>
<p>First this molecule is encoded, then reinforcement learning is done with this scaffold as input. Lastly a new molecule is generated containing this scaffold.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># input is in tutorial/CLI/data/xanthine.tsv</span>
python<span class="w"> </span>-m<span class="w"> </span>drugex.dataset<span class="w"> </span>-b<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span><span class="w"> </span>-i<span class="w"> </span>xanthine.tsv<span class="w"> </span>-mc<span class="w"> </span>SMILES<span class="w"> </span>-o<span class="w"> </span>scaffold_based<span class="w"> </span>-mt<span class="w"> </span>graph<span class="w"> </span>-s
python<span class="w"> </span>-m<span class="w"> </span>drugex.train<span class="w"> </span>-tm<span class="w"> </span>RL<span class="w"> </span>-b<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span><span class="w"> </span>-i<span class="w"> </span>scaffold_based_graph.txt<span class="w"> </span>-o<span class="w"> </span>scaffold_based<span class="w"> </span>-ag<span class="w"> </span>arl_graph_trans_FT<span class="w"> </span>-pr<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span>/models/pretrained/graph-trans/Papyrus05.5_graph_trans_PT/Papyrus05.5_graph_trans_PT.pkg<span class="w"> </span>-p<span class="w"> </span>models/qsar/qspr/models/A2AR_RandomForestClassifier/A2AR_RandomForestClassifier_meta.json<span class="w"> </span>-ta<span class="w"> </span>A2AR_RandomForestClassifier<span class="w"> </span>-sas<span class="w"> </span>-e<span class="w"> </span><span class="m">2</span><span class="w"> </span>-bs<span class="w"> </span><span class="m">32</span><span class="w"> </span>-gpu<span class="w"> </span><span class="m">0</span>
python<span class="w"> </span>-m<span class="w"> </span>drugex.generate<span class="w"> </span>-b<span class="w"> </span><span class="si">${</span><span class="nv">BASE_DIR</span><span class="si">}</span><span class="w"> </span>-i<span class="w"> </span>scaffold_based_graph.txt<span class="w"> </span>-g<span class="w"> </span>scaffold_based_graph_trans_RL<span class="w"> </span>-gpu<span class="w"> </span><span class="m">0</span><span class="w"> </span>-n<span class="w"> </span><span class="m">5</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The not fully converged model here will have trouble producing the scaffold that we need so the generate command may take a long time.</p>
</div>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="install.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="api/modules.html" class="btn btn-neutral float-right" title="drugex" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Xuhan Liu, Sohvi Lukkonen, Helle van den Maagdenberg, Linde Schoenmaker, Martin Šícho, Olivier Béquignon.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>