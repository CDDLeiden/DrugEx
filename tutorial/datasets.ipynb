{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e550ab92-5f0f-45c9-a34b-c69437268441",
   "metadata": {},
   "source": [
    "# Data Sets\n",
    "\n",
    "You have to preprocess the data depending on the model you want to build. You can refer to sections relevant for your model of interest:\n",
    "\n",
    "1. [Graph-Based Transformer](#Preparing-Data-for-the-Graph-Based-Transformer)\n",
    "    * [Scaffold encoding](#Encoding-scaffolds)\n",
    "2. [Smiles-Based Transformer](#Preparing-Data-for-the-Smiles-Based-Transformer) (section incomplete)\n",
    "2. [Smiles-Based Encoder-Decoder](#Preparing-Data-for-the-Smiles-Based-Encoder-Decoder-Model) (section incomplete)\n",
    "2. [Smiles-Based Encoder-Decoder with Attention](#Preparing-Data-for-the-Smiles-Based-Encoder-Decoder-Model-with-Attention) (section incomplete)\n",
    "\n",
    "In this tutorial, we assume you already extracted a list of SMILES strings that you want to use either for pretraining or finetuning. If you want to use the data from this tutorial, you should already have placed the [downloaded example data](https://drive.google.com/file/d/1lYOmQBnAawnDR2Kwcy8yVARQTVzYDelw/view?usp=sharing) in `jupyter/data` (see [README.md](README.md)). For the sake of simplicity, this tutorial uses reduced sets of only 500 compounds (files with the `small` suffix), but the full sets (without a suffix) are also available so that you can build the full models as well.\n",
    "\n",
    "## Loading Molecules\n",
    "\n",
    "Let's start with preprocessing the data from the so called `LIGAND` set that was used in one of the DrugEx publications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2801d9b5-49bc-4200-8010-9eec1f1fa803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    CCCCn1cc2c(nc(NC(=O)Nc3ccc(Cl)c(Cl)c3)n3nc(-c4...\n",
      "1    O=C(Cc1ccccc1)Nc1nc2nn(CCc3ccccc3)cc2c2nc(-c3c...\n",
      "2    O=C(COc1ccccc1)Nc1nc2nn(CCc3ccccc3)cc2c2nc(-c3...\n",
      "3    CC(C)(C)NC(=O)Nc1nc2nn(CCCc3ccccc3)cc2c2nc(-c3...\n",
      "4                COc1ccc(-n2cc3c(n2)c(N)nc2ccccc23)cc1\n",
      "Name: Smiles, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "smiles = pd.read_csv('jupyter/data/LIGAND_RAW_small.tsv', sep='\\t', header=0, usecols=('Smiles',), na_values=('NA', 'nan', 'NaN')).iloc[:,0]\n",
    "smiles.dropna(inplace=True)\n",
    "\n",
    "print(smiles.head())\n",
    "smiles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e8199-c5bc-48db-ad2f-d5859d7e79ad",
   "metadata": {},
   "source": [
    "This set contains the first 500 molecules from the original `jupyter/data/LIGAND_RAW.tsv` file, which contains ligands related to the protein targets of interest in this tutorial, but the full test is also used to train and evaluate models in [the original DrugEx v3 study](https://chemrxiv.org/engage/chemrxiv/article-details/61aa8b58bc299c0b30887f80). These will be our molecules of interest further in the tutorial. They will be used for [finetuning existing pretrained models](finetuning.ipynb). Here, we will create the data sets for this task. Note, that if you are aiming at pretraining your own model the procedure is the same, you just need to use a more general data set (see [pretraining](pretraining.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82805174-377e-409e-8cba-f888243627c0",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "Because we will be potentially processing a lot of data, it might be a good idea to redirect logging outputs to a seperate file so that we keep this notebook clean. DrugEx is using a package-wide logger (available as `drugex.logs.logger`). We can configure it with the standard Python `logging` package. For this tutorial, we already created a method in the `utils` (see [`utils.py`](utils.py)) package that configures a log file for us that will be saved in the `data/logs/` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c97592cc-2a77-43ed-815a-582212787920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import initLogger\n",
    "\n",
    "initLogger('datasets.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54dca55-4f79-4d32-abf6-f3a6bc28eb54",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "The first step in our efforts will be the standardization of the data. That is easily accomplised with the built-in `Standardization` processor that we can apply to our compounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c901b01-be89-480c-9209-761cdd24cb02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b454111138b49cea70674be1bc1db5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Standardizing molecules (batch processing):   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from drugex.data.processing import Standardization\n",
    "\n",
    "N_PROC = 12 # standardization (like many preprocessing tasks in this tutorial) can be done in parallel\n",
    "standardizer = Standardization(n_proc=N_PROC)\n",
    "smiles = standardizer.apply(smiles)\n",
    "\n",
    "len(smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f8b6c9-dc5e-4134-8d5f-bd39c92f8ccf",
   "metadata": {},
   "source": [
    "The standardizer also handles parsing errors for us so the resulting number of molecules can be reduced in comparison to the original data. `Standardization` also allows to change the standardization method by supplying a custom standardizer function (i.e. `Standardization(standardizer=my_fucntion)`). You can find more details in the [documentation](https://martin-sicho.github.io/drugex-docs/api/drugex.data.html?highlight=standardization#drugex.data.processing.Standardization).\n",
    "\n",
    "The standardizer does not handle duplicates so we handle them now. The standardizer should output canonical standardize smiles and, thus, filtering out just duplicate SMILES strings should be sufficient. We do this by creatoing a `set`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37783317-c11a-429a-b0b5-ae0c6040f8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles = set(smiles)\n",
    "len(smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce733b95-9263-4fd6-ae7c-e6ab249bfaac",
   "metadata": {},
   "source": [
    "## Preparing Data for the Graph-Based Transformer\n",
    "\n",
    "The input of the transformer model are the fragments that the molecules of interest are made up of while the molcules of interest themselves are the output. The model then learns to create new valid molecules from the given input fragments. In order to convert the SMILES strings to the encoded model input, we have to generate a so called corpus data set that defines the underlying chemistry or grammar rules for the model. You can use the `FragmentCorpusEncoder` processor in combination with the `GraphFragmentEncoder` to generate the data set for this model:\n",
    "\n",
    "*Note: The term 'corpus' comes from NLP (Natural Language Processing) and was originally used in DrugEx v1 to describe the tokenized SMILES input for the recurrent neural network often used in NLP to represent textual data. We still use the term here for historical reasons even though the graph-based model is a very different type of model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b91a7f-9cf9-4b35-827f-e9abd8892f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from drugex.data.fragments import FragmentCorpusEncoder\n",
    "from drugex.data.fragments import GraphFragmentEncoder, FragmentPairsSplitter\n",
    "from drugex.molecules.converters.fragmenters import Fragmenter\n",
    "from drugex.data.corpus.vocabulary import VocGraph\n",
    "\n",
    "encoder = FragmentCorpusEncoder(\n",
    "    fragmenter=Fragmenter(4, 4, 'brics'), # handles how fragment-molecule pairs are created\n",
    "    encoder=GraphFragmentEncoder(\n",
    "        VocGraph(n_frags=4) # encoder uses the graph vocabulary to create the graph matrix from the created fragment-molecule pairs\n",
    "    ),\n",
    "    pairs_splitter=FragmentPairsSplitter(0.1, 100), # in this instance, we also use a splitter to divide the fragment-molecule pairs into a test set and training set\n",
    "    n_proc=N_PROC # we can again run these actions in parallel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82111a82-52f7-4d43-98ec-bc73e39b5730",
   "metadata": {},
   "source": [
    "When we have defined the encoder (basically a template for data processing), we can just apply it on our data to start encoding (see below). There are two operations involved in this process: \n",
    "\n",
    "1. **Fragmentation** - Determined by the `Fragmenter`, each input molecule is split into fragment-molecule pairs that will form one sample for the model. Depending on the splitting strategy (as determined by `FragmentPairsSplitter`), these pairs are divided into two or more sets. In this instance using the default settings, we collect two data sets in total:\n",
    "\n",
    "    1. **test set** - The set of fragment-molecule pairs used for validation after an epoch of training. Maximum size of this test set is set with `FragmentPairsSplitter`, in this case at most 100 fragment-molecule pairs, but at least 10% of the original data.\n",
    "    2. **trainining set** - It contains all fragment-molecule combinations not selected for the *test set*.\n",
    "\n",
    "2. **Encoding** - This step is handled by the `GraphFragmentEncoder`, which is an implementation of `FragmentEncoder` that is specific to the graph-based model. Using the `VocGraph` vocabulary, it encodes each fragment-molecule pair in the data sets above to a representation understood by the model. This represenation is saved to the resulting `.tsv` files (one per each set after splitting). These files form the `GraphDataSet` and are loaded to the model via a PyTorch `DataLoader`.\n",
    "\n",
    "We initialize the `GraphFragDataSet` instances first with the names of the associated `.tsv` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5621c3ef-4a78-4242-8758-48cf7ad56c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from drugex.data.datasets import GraphFragDataSet\n",
    "import os\n",
    "\n",
    "# create a dedicated directory for our graph data set files\n",
    "graph_input_folder = \"data/sets/graph/\"\n",
    "if not os.path.exists(graph_input_folder):\n",
    "    os.makedirs(graph_input_folder)\n",
    "\n",
    "# create empty data sets (we have to specify a path to a file where the data set will be saved)\n",
    "train = GraphFragDataSet(f\"{graph_input_folder}/ligand_train.tsv\", rewrite=True)\n",
    "test = GraphFragDataSet(f\"{graph_input_folder}/ligand_test.tsv\", rewrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3559733-a3bb-4e7b-ba22-44459f278efa",
   "metadata": {},
   "source": [
    "Now, empty data sets are initialized and if any file already exists it will be overwritten (as set by `rewrite=True`). \n",
    "\n",
    "We can finally run the encoder. We pass our data sets as `encodingCollectors`, which means only the results of the second step described above will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3c1ed13-fbbf-468a-9f6c-7f43a23aac06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03979ff33f54c6c888ceb556b4277ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating fragment-molecule pairs (batch processing):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0603dbe096a47d2a22109aeebcbd123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding fragment-molecule pairs. (batch processing):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191e0bc2cc2743169ce215b205f7ba5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding fragment-molecule pairs. (batch processing):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# apply the encoder and collect data (test data is collected first)\n",
    "encoder.apply(list(smiles), encodingCollectors=[test, train])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31484731-88b0-4d1d-8716-6a557437e963",
   "metadata": {},
   "source": [
    "It is possible some molecules still failed to parse so you can observe this in the logfile to make sure some important patterns were not missed. You can now also check that the appropriate files were indeed created in the `data/sets/graph/` folder. We can easily associate `GraphFragDataSet` instances with these files if we need them again (note that the `rewrite` flag is off by default so the data is not cleared in this case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "407c2ec6-1043-453f-abfb-78fe991d0d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_from_file = GraphFragDataSet('data/sets/graph/ligand_test.tsv')\n",
    "assert os.path.exists(test_from_file.outpath)\n",
    "\n",
    "# we can check the output by converting the data set to a pandas DataFrame again\n",
    "test_from_file.getData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad3a22-de35-45e2-9ca7-3f7ee74cbc03",
   "metadata": {},
   "source": [
    "The `GraphFragDataSet` instance also automatically saves the vocabulary used by the encoder for its creation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a269152-515e-420e-a500-f1c19a09565b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/sets/graph//ligand_train.tsv.vocab'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.getVocPath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4327301f-f0d0-42e0-a932-7a6b9064c341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/sets/graph//ligand_test.tsv.vocab'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.getVocPath()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaac9a65-a9a9-43f9-80bd-95fc013daef9",
   "metadata": {},
   "source": [
    "Specifying a vocabulary is required by the model during training and we can easily reacreate it from these files (see [finetuning](finetuning.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ee0b3-8463-4b93-b3d9-0caaf53505af",
   "metadata": {},
   "source": [
    "## Preparing Data for the Smiles-Based Transformer\n",
    "\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "198949b18dbe1398e4c1e163754b04b8782204695e30d26123ca73588ddf332c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
