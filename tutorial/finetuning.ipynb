{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36debc9f-70a2-4d6b-a314-c59c3599a937",
   "metadata": {},
   "source": [
    "# Finetuning a Pretrained Model\n",
    "\n",
    "The pretrained model usually captures general chemistry rules and as such is best trained on a diverse set of molecules (see [pretraining](./pretraining.ipynb)). However, in drug discovery we usually want to bias such a model to a more focused area of chemical space and that is what happens during finetuning. In this tutorial, we will use the finetuning data sets created [previously](./datasets.ipynb) to bias already existing models. The pretrained models used in this tutorial should be placed in the `jupyter/models/pretrained` folder.\n",
    "\n",
    "## Finetuning a Graph-Based Model\n",
    "\n",
    "The model we will use is pretrained on the CHMEBL database (version 27) with its states saved at `jupyter/models/pretrained/graph/chembl27/chembl27_graph.pkg`. However, before we can load its states the model needs to be initialized so we will need the vocabulary of the model first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674d7d72-51be-4b0d-8bfc-67143541487e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sichom/software/miniconda/envs/drugex-sandbox/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from drugex.training.models.transform import GraphModel\n",
    "from drugex.data.corpus.vocabulary import VocGraph\n",
    "\n",
    "vocabulary = VocGraph.fromFile('jupyter/models/pretrained/graph/chembl27/chembl27_graph_voc.txt')\n",
    "pretrained = GraphModel(voc_trg=vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9849f960-784d-4eef-8f83-5362bc03f2c0",
   "metadata": {},
   "source": [
    "Now we can load the states of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab325824-8ea5-448c-a857-84ac12246c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained.loadStatesFromFile('jupyter/models/pretrained/graph/chembl27/chembl27_graph.pkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11cba8-1767-4cb5-a433-2f1a7005c864",
   "metadata": {},
   "source": [
    "### Finetuning\n",
    "\n",
    "This is not required for pretraining, but it does not hurt to test that the model is working as expected and can generate molecules. \n",
    "\n",
    "First, we need to load our finetuning data we created in the [previous tutorial](datasets.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b84a7a9-83b2-4c50-8d87-72823c8f74cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from drugex.data.datasets import GraphFragDataSet\n",
    "\n",
    "# autoload=True requests that the data from the file is read upon initialization\n",
    "finetuning_data_train = GraphFragDataSet(\"data/model_inputs/graph/ligand_train.tsv\", autoload=True)\n",
    "finetuning_data_test = GraphFragDataSet(\"data/model_inputs/graph/ligand_test.tsv\", autoload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcca7049-1236-4552-b61b-fc82e976fbe5",
   "metadata": {},
   "source": [
    "We can verify the data has been loaded by converting it to pandas `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "716c5539-f31e-416f-b5b1-f03f0c17d12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1165, 400)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuning_data_train.getDataFrame().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab187c01-886e-481f-b5db-1b7f0338bad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 400)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuning_data_test.getDataFrame().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1db42d-85df-430f-b437-2fe09a606bf1",
   "metadata": {},
   "source": [
    "Finally, we can proceed to finetuning the model (note that we need to transform the `DataSet` to `DataLoader` first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b3f92dc-e23a-4074-8952-fe29dfaef372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 10/10 [04:31<00:00, 27.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from drugex.training.trainers import FineTuner\n",
    "from drugex.training.monitors import FileMonitor\n",
    "\n",
    "class SimpleMonitor(FileMonitor):\n",
    "    \"\"\"\n",
    "    Simplified implementation of the FileMonitor. We only need to write basic information for this tutorial.\n",
    "    \"\"\"\n",
    "    \n",
    "    def savePerformanceInfo(self, current_step=None, current_epoch=None, loss=None, *args, loss_valid=None, best=None, **kwargs):\n",
    "        if current_epoch:\n",
    "            self.out.write(f\"Current training loss: {loss} \\n\")\n",
    "            self.out.write(f\"Current validation loss: {loss_valid} \\n\")\n",
    "            self.out.write(f\"\\tBest validation loss: {best} \\n\")\n",
    "            self.out.flush()\n",
    "\n",
    "finetuner = FineTuner(pretrained, gpus=(0,)) # you can specify multiple GPUs to be used with models that support them.\n",
    "monitor = SimpleMonitor('data/models/finetuned/ligand_finetuned')\n",
    "finetuner.fit(\n",
    "    finetuning_data_train.asDataLoader(128),\n",
    "    finetuning_data_test.asDataLoader(128), \n",
    "    epochs=10, # only 10 epochs to speed things up\n",
    "    monitor=monitor\n",
    ")\n",
    "\n",
    "print(\"Finetuning done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced863e-d517-4b33-a8e4-0eaca35942e7",
   "metadata": {},
   "source": [
    "If you want to follow the training progress, you can periodically check the `data/models/finetuned/ligand_finetuned.log` file that is created along with the states of the models by the monitor. The monitor also enables you to get this model's states directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa72cf96-cb73-4249-8b89-ba28825f0d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0205, -0.0666, -0.0142,  ...,  0.0925,  0.0784,  0.0773],\n",
       "        [ 0.0902,  0.0458, -0.0819,  ..., -0.0799,  0.0197,  0.0698],\n",
       "        [ 0.0079, -0.0163,  0.0373,  ..., -0.0541,  0.0064, -0.0043],\n",
       "        ...,\n",
       "        [ 0.0148, -0.0911,  0.1478,  ...,  0.0099,  0.0517, -0.0901],\n",
       "        [ 0.1012,  0.1127, -0.1173,  ...,  0.2809, -0.1795, -0.0868],\n",
       "        [ 0.1036, -0.0344,  0.0282,  ..., -0.0143, -0.0385, -0.1008]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model = monitor.getModel()\n",
    "ft_model['emb_word.weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a178f6-ba1e-439e-a7b8-abb3479c2fe5",
   "metadata": {},
   "source": [
    "So you can use the monitor to initialize another instance easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b82d6630-b4e6-4651-9c55-c7fa6f903c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_model = GraphModel(voc_trg=vocabulary)\n",
    "other_model.load_state_dict(ft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffba80c-e36e-4e43-90f6-8575137517f0",
   "metadata": {},
   "source": [
    "For consistency, we will also save the used vocabulary with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff94625f-c596-4e9e-935c-e934971e2713",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.toFile('data/models/finetuned/ligand_finetuned.vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff63c9e-ffa0-48ec-ab48-e0d0126f4eee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
